<!doctype html><html lang=en><head><title>TCE project: Co-design of application-specific processors with LLVM-based compilation support - The LLVM Project Blog</title>
<meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="LLVM Project News and Details from the Trenches"><meta name=author content="Pekka"><meta property="og:url" content="https://blog.llvm.org/2010/06/tce-project-co-design-of-application.html"><meta property="og:site_name" content="The LLVM Project Blog"><meta property="og:title" content="TCE project: Co-design of application-specific processors with LLVM-based compilation support"><meta property="og:description" content="TTA-based Codesign Environment (TCE) is an application-specific instruction-set processor (ASIP) design toolset developed in Tampere University of Technology in several research projects since 2003. This blog post introduces the project and how LLVM is used in it to provide high-level language compiler support for the designed ASIPs."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2010-06-29T08:38:00+00:00"><meta property="article:modified_time" content="2010-06-29T08:38:00+00:00"><meta property="article:tag" content="Vliw"><meta property="article:tag" content="Llvm-Users"><meta property="article:tag" content="Tce"><meta property="article:tag" content="Asip"><meta property="article:tag" content="Eda"><meta property="article:tag" content="Tta"><meta name=twitter:card content="summary"><meta name=twitter:title content="TCE project: Co-design of application-specific processors with LLVM-based compilation support"><meta name=twitter:description content="TTA-based Codesign Environment (TCE) is an application-specific instruction-set processor (ASIP) design toolset developed in Tampere University of Technology in several research projects since 2003. This blog post introduces the project and how LLVM is used in it to provide high-level language compiler support for the designed ASIPs."><meta name=generator content="Hugo 0.128.0"><link rel=stylesheet href=https://blog.llvm.org/css/normalize.min.css><link rel=stylesheet href=https://blog.llvm.org/fontawesome/css/all.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto+Slab|Ruda"><link rel=stylesheet type=text/css href=https://blog.llvm.org/css/styles.css></head><body><div id=container><header><h1><a href=https://blog.llvm.org/>The LLVM Project Blog</a></h1><ul id=social-media><li><a href=https://www.facebook.com/llvmorg title=Facebook><i class="fab fa-facebook fa-lg"></i></a></li><li><a href=https://github.com/llvm title=GitHub><i class="fab fa-github fa-lg"></i></a></li><li><a href=https://twitter.com/llvmorg title=Twitter><i class="fab fa-twitter fa-lg"></i></a></li><li><a href=https://www.youtube.com/c/LLVMPROJ title=Youtube><i class="fab fa-youtube fa-lg"></i></a></li></ul><p><em>LLVM Project News and Details from the Trenches</em></p></header><nav><ul><li><a href=https://blog.llvm.org/about><i class="fa-li fa fa-lg"></i><span>About</span></a></li><li><a href=https://blog.llvm.org/posts><i class="fa-li fa fa-lg"></i><span>Posts</span></a></li><li><a href=https://blog.llvm.org/tags><i class="fa-li fa fa-lg"></i><span>Tags</span></a></li><li><a href=https://llvm.org/><i class="fa-li fa fa-lg"></i><span>llvm.org</span></a></li></ul></nav><main><article><h1>TCE project: Co-design of application-specific processors with LLVM-based compilation support</h1><aside><ul><li><time class=post-date datetime=2010-06-29T08:38:00Z>Jun 29, 2010</time></li><li><em><a href=https://blog.llvm.org/tags/vliw>#vliw</a>
,
<a href=https://blog.llvm.org/tags/llvm-users>#llvm-users</a>
,
<a href=https://blog.llvm.org/tags/tce>#tce</a>
,
<a href=https://blog.llvm.org/tags/asip>#asip</a>
,
<a href=https://blog.llvm.org/tags/eda>#eda</a>
,
<a href=https://blog.llvm.org/tags/tta>#tta</a></em></li><li>10 minute read</li></ul></aside><a href=http://tce.cs.tut.fi/>TTA-based Codesign Environment</a> (TCE) is an application-specific instruction-set processor (ASIP) design toolset developed in <a href=http://www.tut.fi/>Tampere University of Technology</a> in several research projects since 2003. This blog post introduces the project and how LLVM is used in it to provide high-level language compiler support for the designed ASIPs.<br><br><a name=more></a><br><br><br><span style=font-size:medium>The use case for application-specific processors</span><br><br>Especially in embedded devices so called general purpose "off-the-shelf processors" are often not optimal for the application at hand. The readily available processors might be too large in chip area, might consume too much power, might be too expensive (for a mass product) or are not running the set of programs fast enough. In order to tackle the performance problem, a common design choice is to run parts of the application in software while speeding up the performance critical functions with a custom hand-tailored hardware accelerator or a co-processor implemented as an <a href=http://en.wikipedia.org/wiki/Application-specific_integrated_circuit>application-specific integrated circuit</a> (ASIC).<br><br>The design, implementation and verification time of the hardware accelerators costs money and lengthens the time-to-market for the designed device. In addition, a fixed function hardware accelerator designed with an hardware description language (HDL) such as VHDL or Verilog has the problem of being "carved in stone", thus not providing programmability that enables late bug fixes and on-the-field updates to the supported set of functions.<br><br><a href=http://en.wikipedia.org/wiki/Fpga>Field-Programmable Gate Arrays</a> (FPGAs) allow reconfiguring the implemented hardware logic on the field. However, as FPGA is only an hardware design implementation technique, the "<a href=http://en.wikipedia.org/wiki/Non-recurring_engineering>non-recurring engineering cost</a>" of designing the hardware logic in an HDL is still there.<br><br>Application-specific processors can be spotted in the "design space" between off-the-shelf processors, such as ARM products or Texas Instruments DSPs where functionality is described fully in software by the designer, and custom fixed function hardware accelerators where functionality is described fully in hardware description language by the designer. In case of ASIPs, the engineer is able to design both software and hardware at the same time (co-design) and is free to the choose the level of application specialization applied to the processor.<br><br>What can be customized in an ASIP depends on the used processor template. A commonly customized part of the processor is the instruction set. Customizable instruction set allows the designer to define new application specific instructions for the processor to implement the desired functionality faster than a set of basic software operations such as additions or shifts can. Examples of such special instructions include complex arithmetic, non-standard floating point arithmetic, application-specific precision fixed point arithmetic, more than two input adders, etc.<br><br>TCE places few restrictions on the types of custom instructions that can be added to the processor design. For example, there are no limits to the number of input operands or produced results nor the number of clock cycles the operation can execute. In addition to the custom operations, the number and size of the register files (RF), the number and type of the functional units (FU) and the connectivity between the RFs and FUs can be freely customized.<br><br><span style=font-size:medium>About Transport Triggered Architectures</span><br><br>TCE is based on a simple but scalable architecture template called <a href=http://en.wikipedia.org/wiki/Transport_triggered_architecture>Transport Triggered Architecture</a> (TTA, not to be confused with "Time Triggered Architecture"). TTA can be described as an exposed datapath VLIW architecture. The "exposed datapath" part means that the data transports that take place between the functional units (e.g. arithmetic logic unit or a multiplier) and the register files are explicitly visible to the programmer. In other words, while processors are commonly programmed by defining which operations to execute (including information of the sources of the operands and the destinations of the results), TTA is programmed by defining the transports of the operands and results. The name TTA comes from the way operations are executed: when operand data is moved to the triggering port of the functional unit, the operation starts executing. After a fixed latency (from the architecture point of view) the results can be read from the output ports of the functional unit to the next destination.<br><br>The programming model can be illustrated more easily with an assembly code snippet example.<br><br>1) <span style=font-style:italic>Traditional "operation triggered" (first parameter is the destination)</span><br><br><ol><li>ADD R1, R2, R3</li><li>MUL R4, R1, R5<br></li></ol><br>2) <span style=font-style:italic>Transport triggered</span><br><br><ol><li>R2 -> ADD.OPERAND, R3 -> ADD.TRIGGER</li><li>ADD.RESULT -> R1</li><li>R1 -> MUL.OPERAND, R5 -> MUL.TRIGGER</li><li>MUL.RESULT -> R4<br></li></ol>Transport programming enables some specific software optimizations such as software (register) bypassing which in turn can enable "dead result read elimination", that can be applied in case all result reads can be transported directly to the destination functional units. This can lead to reduced register (file) pressure. An example follows:<br><br>3. <span style=font-style:italic>Transport triggered with software bypassing and dead result read elimination</span><br><br><ol><li>R2 -> ADD.OPERAND, R3 -> ADD.TRIGGER</li><li>ADD.RESULT -> MUL.OPERAND, R5 -> MUL.TRIGGER</li><li>MUL.RESULT -> R4<br></li></ol>Here the result from the adder is copied directly to the operand input port of the multiplier, thus removing the need for the general purpose register R1 used as a temporary storage. In addition to reducing register pressure, the freedom to schedule the movement of operand/result data in multiple cycles reduces the register file port pressure, which is one of the TTA's main motivations. In the traditional VLIW the number of RF ports needs to be scaled according to the number of connected functional units and their worst case RF port requirements (maximum simultaneous operand reads and result writes), leading to more complex RFs with higher delay and area.<br><br><span style=font-size:medium>Toolset assisted processor design with TCE</span><br><br>Designing new processors from the scratch is not a straightforward task. One needs to take care of the design, verification and porting a high level language programming toolchain for each of the processors so the programmers are happy (writing peculiar assembler syntax for a changing target would get quite depressing quickly!). Thus, the design process should be automated as fully as possible to make experimenting with different processor architecture alternatives feasible.<br><br>The ultimate goal for an ASIP design toolset is to be as easy to use as taking a high-level language program as input and producing as a result an optimal processor implementation in VHDL or Verilog. It should parallelize the program for the processor's resources efficiently while exploiting custom instructions intelligently without any user intervention. In our experience, this type of fully automated "design space exploration" tends not to produce good enough results as the codesign process is often something that a human can do more efficiently. For example, sometimes the software needs to be refactored to a form that can exploit instruction level parallelism better. Sometimes it can be hard, or even impossible, for a software algorithm to realize that a complex-looking loop can be replaced with a simple single cycle custom instruction if implemented in hardware, and so on. Thus, we see that the realistic use case for an ASIP design toolset is to assist in the design task as much as possible while still leaving leeway for the engineer to exploit their knowledge in the field of algorithms or hardware design. This way, in case the engineer is skilled enough and the toolset assisting the ASIP design task is flexible enough, the processor design can eventually reach the performance of a fixed function hardware accelerator, while the design process can also be stopped at any point when the result is good enough.<br><br>TCE is at a relatively mature state, providing graphical tools to design the TTA architectures, architecture description driven instruction set simulators, a retargetable compiler, and a processor implementation generator supporting VHDL output. Because TCE uses TTA, a static ILP architecture, as its processor template, the efficiency of the end result is highly dependent on an efficient compiler. The compiler has been our main focus in recent years and will most likely be in the future also.<br><br><span style=font-size:medium>LLVM in TCE</span><br><br>We were introduced to the LLVM project at about 2006. Until that point we used an old gcc v2.7.0 compiler ported from MOVE, the toolset preceeding TCE. It goes without saying that maintaining such an ancient piece of gcc code was quite a challenge and we actively tried to look for something easier to work with.<br><br>In addition to a clean C++-based code base, one of the main things that lured us towards LLVM from a purely gcc-based compiler were the interprocedural optimizations. Interprocedural optimizations are very useful for us as we work with standalone (no operating system with a runtime linker assumed) fully linked programs. In fact, our compiler toolchain does not currently include any linker at all but inputs fully linked LLVM bitcodes to its code generation phase. This means that most programs benefit from the LLVM global optimizations such as aggressive inlining and dead code elimination as the externally visible interface in the compiled programs can be limited merely to the startup functions.<br><br>After poking around and doing some experimental TTA code generation prototyping with LLVM (I think LLVM was at version 1.7 or so at that point) we noted that LLVM was getting more and more traction and started to look for a proper way for using LLVM for parts of the TCE code generation process.<br><br>As described previously, our target, on top of being customizable, is transport triggered, thus nothing like any other architecture supported by LLVM. This caused some trial-and-error coding efforts while figuring out the best way of taking advantage of the existing code base of LLVM while still supporting instruction scheduling that exploits the special trickery and the extra scheduling freedom enabled by TTA. Another requirement, the automated retargeting of the LLVM backend for the resources of different designed processors caused some long work days to get functioning robustly.<br><br>In the end, we came up with a plugin based backend approach. TCE code generation chain now includes a tool that generates LLVM backends from our XML-format architecture description files, compiles them with a C++ compiler to a dynamic library, and loads the generated LLVM backend on the fly during the code generation. At this point each TTA is modeled as a simple operation triggered architecture so the supported instruction set and registers can be described in the TableGen format. This way we could use the LLVM instruction selection and register allocation code, but the resulting sequential code is not yet something we can execute in a TTA. To finalize the code generation, after LLVM register allocation we convert the MachineInstructions to another internal representation (CFG+DDG with moves as the graph nodes) and do the rest of the code generation and TTA-specific optimizations on the TCE side.<br><br>The main missing piece for us in LLVM code generation framework is a VLIW-type instruction scheduler. In case of TTA which is a statically scheduled architecture with programmer visible operation latencies, the compiler instruction scheduling is not merely an optimization that can be optionally executed to get more performance. It's fully up to the compiler to schedule the operations in such a way that the operations are not started too early (in case a previous operation is executing) nor the results read too soon (in case the operation has not yet finished). Thus, in our case, the instruction scheduling is actually mandatory to get correct results. Also, in order to exploit the instruction level parallelism in an architecture like TTA or VLIW, one needs a way to bundle multiple instructions (in our case data transport moves) in a single wide instruction (or "a cycle") to be executed in parallel. Finally, a way to model the processor resources (with resource tables or similar) during scheduling is needed to produce correct code in the absence of structural hazard detection.<br><br>It would be really nice to have such a VLIW-style scheduling framework in LLVM so we could move more of our code generation to LLVM-side. Unfortunately due to a lack of time, we have not yet started to work on this as our existing instruction scheduler does the job well enough when starting from the "sequential RISC-like operation triggered" output from the LLVM code generation.<br><br>In general, LLVM and it core developers have been a pleasure to work with and we hope to be able to contribute to the LLVM project more in the future. Keep up the good work!<br><br><span style=font-size:medium>Future work and final words</span><br><br>Currently we are looking into GPGPU-style workload compilation issues. We are experimenting with OpenCL to describe the applications for easier extraction of parallelism while still providing clean support for calling custom operations from the kernel code. There is also work ongoing to extend TCE to better support task level parallelism with multicore ASIP generation and compiler assisted multithreading.<br><br>In case you got interested in the project or have questions to ask, please join the mailing list <a href=http://tce.cs.tut.fi/index.php/discussion>tce-users@cs.tut.fi</a>.<br><br>I hope to see you there!<br><br><span style=font-style:italic>--Pekka Jääskeläinen,</span><br><span style=font-style:italic;font-size:x-small>a researcher that has been working in the TCE project from the start.</span><br><br><span style=font-size:medium></span></article><section class=post-nav><ul><li><a href=https://blog.llvm.org/2010/06/new-lldb-debugger.html><i class="fa fa-chevron-circle-left"></i> New "lldb" Debugger</a></li><li><a href=https://blog.llvm.org/2010/09/llvm-in-clamav.html>LLVM in ClamAV <i class="fa fa-chevron-circle-right"></i></a></li></ul></section></main><footer><ul><li><h6>llvm.org |
Rendered by <a href=https://gohugo.io title=Hugo>Hugo</a> |
<a href=https://blog.llvm.org/index.xml>Subscribe</a></h6></li></ul></footer></div><script src=https://blog.llvm.org/js/scripts.js></script></body></html>