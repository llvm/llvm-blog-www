---
author: Kristof Beyls
blogger_id: tag:blogger.com,1999:blog-6088150582281556517.post-5962036805506036486
blogger_orig_url: http://blog.llvm.org/2016/06/using-lnt-to-track-performance.html
url: "2016/06/using-lnt-to-track-performance.html"
date: "2016-06-15T23:19:00Z"
modified_time: "2016-06-15T23:19:41.192-07:00"
tags: ["LNT"]
thumbnail: https://4.bp.blogspot.com/-lNSURt-I4tc/V2F48OowcSI/AAAAAAAAFg8/EYJXuOe9tZ4_ljn2fzbEjcdTArs0BAeDACLcB/s72-c/lnt_daily_report.png
title: Using LNT to Track Performance
aliases:
 - /2016/06/using-lnt-to-track-performance.html
---

<span style="font-family: inherit;"><br /></span><span style="font-family: inherit;">In the past year, LNT has grown a number of new features that makes performance tracking and understanding the root causes of performance deltas a lot easier. In this post, I’m showing how we’re using these features.</span><br /><span style="font-family: inherit;"><br /></span><span style="font-family: inherit;">LNT contains 2 big pieces of functionality:</span><br /><ol><li><span style="font-family: inherit;">A server,<br />a. to which you can submit correctness and performance measurement data, by sending it a json-file in the correct format,<br />b. that analyzes which performance changes are significant and which ones aren't,<br />c. that has a webui to show results and analyses in a number of different ways.</span></li><li><span style="font-family: inherit;">A command line tool to run tests and benchmarks, such as LLVM’s test-suite, SPEC2000 and SPEC2006 benchmarks.</span></li></ol><span style="font-family: inherit;">This post focuses on using the server. None of the features I’ll show are LLVM-specific, or even specific to ahead-of-time code generators, so you should be able to use LNT in the same way for all your code performance tracking needs. At the end, I’ll give pointers to the documentation needed to setup an LNT server and how to construct the json file format with benchmarking and profiling data to be submitted to the server.</span><br /><span style="font-family: inherit;">The features highlighted focus on tracking the performance of code, not on other aspects LNT can track and analyze.</span><br /><span style="font-family: inherit;">We have 2 main uses cases in tracking performance:</span><br /><ul><li><span style="font-family: inherit;">Post-commit detection of performance regressions and improvements.</span></li><li><span style="font-family: inherit;">Pre-commit analysis of the impact of a patch on performance.</span></li></ul><span style="font-family: inherit;">I'll focus on the post-commit detection use case.</span><br /><h2><span style="font-family: inherit;">Post-commit performance tracking</span></h2><h4>Step 1. Get an overview of the "Daily Report" page</h4><div><div>Assuming your server runs at http://yourlntserver:8000, this page is located at http://yourlntserver:8000/db_default/v4/nts/daily_report</div><div>The page gives a summary of the significant changes it found today.</div><div>An example of the kind of view you can get on that page is the following</div></div><div class="separator" style="clear: both; text-align: center;"><a href="https://4.bp.blogspot.com/-lNSURt-I4tc/V2F48OowcSI/AAAAAAAAFg8/EYJXuOe9tZ4_ljn2fzbEjcdTArs0BAeDACLcB/s1600/lnt_daily_report.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="256" src="https://4.bp.blogspot.com/-lNSURt-I4tc/V2F48OowcSI/AAAAAAAAFg8/EYJXuOe9tZ4_ljn2fzbEjcdTArs0BAeDACLcB/s640/lnt_daily_report.png" width="640" /></a></div><div class="separator" style="clear: both;">In the above screenshot, you can see that there were performance differences on 3 different programs, bigfib, fasta and ffbench. The improvement on ffbench only shows up on a machine named “machine3”, whereas the performance regression on the other 2 programs shows up on multiple machines.</div><div class="separator" style="clear: both;"><br /></div><div class="separator" style="clear: both;">The table shows how performance evolved over the past 7 days, one column for each day. The sparkline on the right shows graphically how performance has evolved over those days. When the program was run multiple times to get multiple sample points, these show as separate dots that are vertically aligned (because they happened on the same date). The background color in the sparkline represents a hash of the program binary. If the color is the same on multiple days, the binaries were identical on those days.</div><div class="separator" style="clear: both;"><br /></div><div class="separator" style="clear: both;">Let’s look first at the ffbench program. The background color in the sparkline is the same for the last 2 days, so the binary for this program didn’t change in those 2 days. Conclusion: the reported performance variation of -8.23% is caused by noise on the machine, not due to a change in code. The vertically spread out dots also indicate that this program has been noisy consistently over the past 7 days.</div><div class="separator" style="clear: both;"><br /></div><div class="separator" style="clear: both;">Let’s now look at the bigfib. The background color in the sparkline has changed since its previous run, so let’s investigate further. By clicking on one of the machine names in the table, we go to a chart showing the long-term evolution of the performance of this program on that machine.</div><h4>Step 2. The long-term performance evolution chart</h4><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-S53qGJAhZiI/V2F58jQ0NVI/AAAAAAAAFhI/XuxqfUnh0rshfgqD1Gcn1SEynH22ZOEKACLcB/s1600/longterm-perf-evolution-chart.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="280" src="https://1.bp.blogspot.com/-S53qGJAhZiI/V2F58jQ0NVI/AAAAAAAAFhI/XuxqfUnh0rshfgqD1Gcn1SEynH22ZOEKACLcB/s640/longterm-perf-evolution-chart.png" width="640" /></a></div><div class="separator" style="clear: both;">This view shows how performance has evolved for this program since we started measuring it. When you click on one of the dots, which each represent a single execution of the program, you get a pop-up with information such as revision, date at which this was run etc.</div><div class="separator" style="clear: both;">When you click on the number after “Run:” in that pop-up, it’ll bring you to the run page.</div><h4 style="clear: both;">Step 3. The Run page</h4><div><div>The run page gives an overview of a full “Run” on a given machine. Exactly what a Run contains depends a bit on how you organize the data, but typically it consists of many programs being run a few times on 1 machine, representing the quality of the code generated by a specific revision of the compiler on one machine, for one optimization level.</div><div>This run page shows a lot of information, including performance changes seen since the previous run:</div></div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-2CaZGoSAmL4/V2F7Fftk8XI/AAAAAAAAFhU/gnTa49e6lt0ckVLkj49zP20in0VvXd5HACLcB/s1600/run-over-run%2Bchanges.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="128" src="https://1.bp.blogspot.com/-2CaZGoSAmL4/V2F7Fftk8XI/AAAAAAAAFhU/gnTa49e6lt0ckVLkj49zP20in0VvXd5HACLcB/s640/run-over-run%2Bchanges.png" width="640" /></a></div><div>When hovering with the mouse over entries, a “Profile” button will show, that when clicked, shows profiles of both the previous run and the current run.</div><h4>Step 4. The Profile page</h4><div><div>At the top, the page gives you an overview of differences of recorded performance events between the current and previous run.</div></div><div class="separator" style="clear: both; text-align: center;"><a href="https://2.bp.blogspot.com/-1KaKjIGohZw/V2F9WxfRdII/AAAAAAAAFhg/d1ePCRy_uL4T6rVQnZPk316H8iuxutQ0wCLcB/s1600/PerformanceCounters.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="64" src="https://2.bp.blogspot.com/-1KaKjIGohZw/V2F9WxfRdII/AAAAAAAAFhg/d1ePCRy_uL4T6rVQnZPk316H8iuxutQ0wCLcB/s640/PerformanceCounters.png" width="640" /></a></div><div class="separator" style="clear: both; text-align: left;">After selecting which function you want to compare, this page shows you the annotated assembly:</div><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-k5mJCSu02Og/V2F90A9uiSI/AAAAAAAAFho/epjri2UOIPMYQILotrAX34gllJWoSBMSwCLcB/s1600/profile_view.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="306" src="https://1.bp.blogspot.com/-k5mJCSu02Og/V2F90A9uiSI/AAAAAAAAFho/epjri2UOIPMYQILotrAX34gllJWoSBMSwCLcB/s640/profile_view.png" width="640" /></a></div><div class="separator" style="clear: both; text-align: left;"><br /></div><div><br /></div><div><div>While it’s clear that there are differences between the disassembly, it’s often much easier to understand the differences by reconstructing the control flow graph to get a per-basic-block view of differences. By clicking on the “View:” drop-down box and selecting the assembly language you see, you can get a CFG view. I find showing absolute values rather than relative values helps to understand performance differences better, so I also chose “Absolute numbers” in the drop down box on the far right:</div><div class="separator" style="clear: both; text-align: center;"><a href="https://4.bp.blogspot.com/-intetvmwToI/V2F-WH08lII/AAAAAAAAFh0/1rNkIiUEtOAtQ1QeWVaSXkXExXjK-8rqACLcB/s1600/profile_view_cfg.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="196" src="https://4.bp.blogspot.com/-intetvmwToI/V2F-WH08lII/AAAAAAAAFh0/1rNkIiUEtOAtQ1QeWVaSXkXExXjK-8rqACLcB/s640/profile_view_cfg.png" width="640" /></a></div><div class="separator" style="clear: both;">There is obviously a single hot basic block, and there are differences in instructions in the 2 versions. The number in the red side-bar shows that the number of cycles spent in this basic block has increased from 431M to 716M. In just a few clicks, I managed to drill down to the key codegen change that caused the performance difference!</div><div class="separator" style="clear: both;"><br /></div><div class="separator" style="clear: both;">We combine the above workflow with the llvmbisect tool available at <a href="http://llvm.org/viewvc/llvm-project/zorg/trunk/llvmbisect/">http://llvm.org/viewvc/llvm-project/zorg/trunk/llvmbisect/</a> to also quickly find the commit introducing the performance difference. We find that using both the above LNT workflow and the llvmbisect tool are vital to be able to act quickly on performance deltas.</div><h2>Pointers on setting up your own LNT server for tracking performance</h2><div><div>Setting up an LNT server is as simple as running the half a dozen commands documented at <a href="http://lnt.llvm.org/quickstart.html">http://lnt.llvm.org/quickstart.html</a> under "Installation" and "Viewing Results". The "Running tests" section is specific to LLVM tests, the rest is generic to performance tracking of general software.</div><div><br /></div><div>The documentation for the json file format to submit results to the LNT server is here: <a href="http://lnt.llvm.org/importing_data.html">http://lnt.llvm.org/importing_data.html</a>.</div><div>The documentation for how to also add profile information, is at <a href="http://lnt.llvm.org/profiles.html">http://lnt.llvm.org/profiles.html</a>.</div></div><div><br /></div></div><div><br /></div>
