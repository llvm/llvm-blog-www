<!doctype html><html lang=en><head><title>Advanced Topics in Redundant Load Elimination with a Focus on PHI Translation - The LLVM Project Blog</title>
<meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="LLVM Project News and Details from the Trenches"><meta name=author content="Chris Lattner"><meta property="og:url" content="https://blog.llvm.org/2009/12/advanced-topics-in-redundant-load.html"><meta property="og:site_name" content="The LLVM Project Blog"><meta property="og:title" content="Advanced Topics in Redundant Load Elimination with a Focus on PHI Translation"><meta property="og:description" content="In our previous post on GVN we introduced some basics of load elimination.  This post describes some advanced topics and focuses on PHI translation: what it is, why it is important, shows some nice things it can do, and describes the implementation in LLVM."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2009-12-19T14:45:00+00:00"><meta property="article:modified_time" content="2009-12-19T14:45:00+00:00"><meta property="article:tag" content="Optimization"><meta property="article:tag" content="New-in-Llvm-2.7"><meta name=twitter:card content="summary"><meta name=twitter:title content="Advanced Topics in Redundant Load Elimination with a Focus on PHI Translation"><meta name=twitter:description content="In our previous post on GVN we introduced some basics of load elimination.  This post describes some advanced topics and focuses on PHI translation: what it is, why it is important, shows some nice things it can do, and describes the implementation in LLVM."><meta name=generator content="Hugo 0.128.0"><link rel=stylesheet href=https://blog.llvm.org/css/normalize.min.css><link rel=stylesheet href=https://blog.llvm.org/fontawesome/css/all.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto+Slab|Ruda"><link rel=stylesheet type=text/css href=https://blog.llvm.org/css/styles.css></head><body><div id=container><header><h1><a href=https://blog.llvm.org/>The LLVM Project Blog</a></h1><ul id=social-media><li><a href=https://www.facebook.com/llvmorg title=Facebook><i class="fab fa-facebook fa-lg"></i></a></li><li><a href=https://github.com/llvm title=GitHub><i class="fab fa-github fa-lg"></i></a></li><li><a href=https://twitter.com/llvmorg title=Twitter><i class="fab fa-twitter fa-lg"></i></a></li><li><a href=https://www.youtube.com/c/LLVMPROJ title=Youtube><i class="fab fa-youtube fa-lg"></i></a></li></ul><p><em>LLVM Project News and Details from the Trenches</em></p></header><nav><ul><li><a href=https://blog.llvm.org/about><i class="fa-li fa fa-lg"></i><span>About</span></a></li><li><a href=https://blog.llvm.org/posts><i class="fa-li fa fa-lg"></i><span>Posts</span></a></li><li><a href=https://blog.llvm.org/tags><i class="fa-li fa fa-lg"></i><span>Tags</span></a></li><li><a href=https://llvm.org/><i class="fa-li fa fa-lg"></i><span>llvm.org</span></a></li></ul></nav><main><article><h1>Advanced Topics in Redundant Load Elimination with a Focus on PHI Translation</h1><aside><ul><li><time class=post-date datetime=2009-12-19T14:45:00Z>Dec 19, 2009</time></li><li><em><a href=https://blog.llvm.org/tags/optimization>#optimization</a>
,
<a href=https://blog.llvm.org/tags/new-in-llvm-2.7>#new-in-llvm-2.7</a></em></li><li>14 minute read</li></ul></aside>In our <a href=http://blog.llvm.org/2009/12/introduction-to-load-elimination-in-gvn.html>previous post on GVN</a> we introduced some basics of load elimination. &nbsp;This post describes some advanced topics and focuses on PHI translation: what it is, why it is important, shows some nice things it can do, and describes the implementation in LLVM.<br><a name=more></a><br><span style=font-size:x-large>What is PHI Translation All About?</span><br><br>When performing redundancy elimination on SSA form,&nbsp;PHI translation is required when we are walking through the CFG and the value being analyzed has an operand defined in the block we're walking through. &nbsp;In the case of load elimination, the operand of the value being analyzed is the pointer we're loading from. This means that PHI translation happens when the pointer we're loading is defined in a block. For example, consider this C code:<br><br><pre>if (...) {<br>    *A = 5;<br>    P = A;<br>  } else {<br>    *B = 9;<br>    P = B;<br>  }<br>  use(*P);<br></pre><br>This compiles into LLVM IR that looks like this:<br><br><pre>BB1:<br>&nbsp;&nbsp;store i32 5, i32* %A<br>&nbsp;&nbsp;br label %Merge<br><br>BB2:<br>&nbsp;&nbsp;store i32 9, i32* %B<br>&nbsp;&nbsp;br label %Merge<br><br>Merge:<br>&nbsp;&nbsp;%P = phi i32* [%A, %BB1], [%B, %BB2]<br>&nbsp;&nbsp;%Y = load i32 *%P<br>&nbsp;&nbsp;...<br>&nbsp;&nbsp;... use %Y ...<br></pre><br>In this example, GVN starts looking at the %Y load to see if it can eliminate it. &nbsp;It scans backwards up the block, looking for other things that provide the value of the memory address at %P. &nbsp;Unlike in the example in the previous post, here we actually find a definition of %P: what do we do?<br><br>In LLVM 2.6 and before, memory dependence analysis would just give up and act as though the memory had been clobbered, preventing the load from being analyzed further (and eventually being eliminated in this case). &nbsp;New in LLVM 2.7, GVN is able to translate %P into each predecessor. &nbsp;In this case, it sees that %P has the value of %A in %BB1 (and then sees that %A is available there) and that %P has the value of %B in %BB2 (where it is also available). This allows GVN to optimize this example into:<br><br><pre>BB1:<br>&nbsp;&nbsp;store i32 5, i32* %A<br>&nbsp;&nbsp;br label %Merge<br><br>BB2:<br>&nbsp;&nbsp;store i32 9, i32* %B</pre><pre>&nbsp;&nbsp;br label %Merge<br><br>Merge:<br>&nbsp;&nbsp;%Y = phi i32 [5, %BB1], [9, %BB2]<br>&nbsp;&nbsp;%P = phi i32* [%A, %BB1], [%B, %BB2]<br>&nbsp;&nbsp;...<br>&nbsp;&nbsp;... use %Y ...<br></pre><br>which eliminates the load. This post talks about how this is done, the nuances involved in getting this right, and what this allows us to optimize.<br><br><span style=font-size:x-large>Why is PHI Translation Required for Correctness?</span><br><br>One thing that wasn't obvious to me when I dove into this is that PHI translation is actually required for correctness. &nbsp;This is the reason why memory dependence analysis must stop scanning if it finds the definition of the pointer. &nbsp;The cases where this happen are somewhat subtle and usually involve partial redundancy elimination. &nbsp;For example, consider this loop:<br><br><pre>Loop:<br>  %P = phi i32* [%A, %Entry], [%B, %Loop]<br>  %X = load i32* %P<br>  ...<br>  store i32 5, i32* %B<br>  store i32 4, i32* %P<br>  br i1 %cond, label %Loop, label %Out<br></pre><br>Consider what would happen if PHI translation kept scanning for %P in predecessors as it went up the CFG: it would scan from the %X load up to the top of the Loop block. If not doing PHI translation correctly, it would continue scanning for %P in the Entry block (which we ignore in this example), then scan for %P in the Loop block (which is a predecessor of itself).<br><br>Scanning for %P in the loop block finds the store of 4 to %P, so we'd consider 4 to be a live-in value on the edge from loop. However, this is <b>incorrect</b> and will lead to a miscompilation: the actual value live on that backedge is 5, which is provided by the previous store.<br><br>This simple example is intended to show that PHI translation is not an optimization: it is required for correctness, and that the most simple form of PHI translation is to give up when you see a definition of the address. However, LLVM recently got smart enough to do much better than this or the simple pointer case above.<br><br><span style=font-size:x-large>A More Interesting Case</span><br><br>Consider this test case:<br><br><pre>struct S { int X, Y; };<br><br>int foo(struct S *P1, struct S *P2, int C) {<br>  struct S *P;<br>  if (C) {<br>    P1-&gt;X = 4;<br>    P1-&gt;Y = 2;<br>    P = P1;<br>  } else {<br>    P2-&gt;X = 24;<br>    P2-&gt;Y = 2;<br>    P = P2;<br>  }<br>  return P-&gt;X + P-&gt;Y;<br>} <br></pre><br>This code compiles down to IR that looks like this (using "<tt>clang t.c -S -o - -emit-llvm | opt -mem2reg -S</tt>":<br><br><pre>if.then:<br>  %tmp2 = getelementptr inbounds %struct.S* %P1, i32 0, i32 0<br>  store i32 4, i32* %tmp2<br>  %tmp4 = getelementptr inbounds %struct.S* %P1, i32 0, i32 1<br>  store i32 2, i32* %tmp4<br>  br label %if.end<br><br>if.else: <br>  %tmp7 = getelementptr inbounds %struct.S* %P2, i32 0, i32 0<br>  store i32 24, i32* %tmp7<br>  %tmp9 = getelementptr inbounds %struct.S* %P2, i32 0, i32 1 <br>  store i32 2, i32* %tmp9<br>  br label %if.end<br><br>if.end: <br>  %P = phi %struct.S* [ %P1, %if.then ], [ %P2, %if.else ] <br>  %tmp12 = getelementptr inbounds %struct.S* %P, i32 0, i32 0 <br>  %tmp13 = load i32* %tmp12 <br>  %tmp15 = getelementptr inbounds %struct.S* %P, i32 0, i32 1 <br>  %tmp16 = load i32* %tmp15 <br>  %add = add nsw i32 %tmp13, %tmp16 <br>  ret i32 %add<br>}<br></pre><br>In this case, GVN looks to eliminate the %tmp13 and %tmp14 loads. &nbsp;Consider the %tmp13 load: it scans backwards from the load, looking for available values of %tmp12. &nbsp;As it goes, it immediately finds the definition of the pointer (%tmp12), so it needs to PHI translate the pointer or give up. &nbsp;Without getting into the details yet of how this is done, here is the intuition of what happens:<br><br>In this case, the pointer is not a PHI node, but it <b>uses</b> a PHI node as an operand. &nbsp;Because of this, GVN needs to phi translate the entire symbolic expression "gep P, 0, 0" into the predecessors. &nbsp;It does this, forming the symbolic expression "gep P1, 0, 0" and it finds that the phi translated address is available as %tmp2 in the %if.then block. &nbsp;It then PHI translates the "gep P, 0, 0" expression into the %if.else forming the "gep P2, 0, 0" symbolic expression and finds that it is available as %tmp7. &nbsp;It scans those blocks for the pointers and finds that the values are, in fact, available. &nbsp;Because they are both available, it can use insert construct SSA form to eliminate the load.<br><br>This allows it to see the both loads are in fact available in the predecessors. &nbsp;Here is the code after GVN (using "<tt>clang t.c -S -o - -emit-llvm | opt -mem2reg -S -gvn -die</tt>):<br><br><pre>if.then:<br>  %tmp2 = getelementptr inbounds %struct.S* %P1, i32 0, i32 0<br>  store i32 4, i32* %tmp2<br>  %tmp4 = getelementptr inbounds %struct.S* %P1, i32 0, i32 1<br>  store i32 2, i32* %tmp4<br>  br label %if.end<br><br>if.else:<br>  %tmp7 = getelementptr inbounds %struct.S* %P2, i32 0, i32 0<br>  store i32 24, i32* %tmp7<br>  %tmp9 = getelementptr inbounds %struct.S* %P2, i32 0, i32 1<br>  store i32 2, i32* %tmp9<br>  br label %if.end<br><br>if.end:<br>  %tmp13 = phi i32 [ 1, %if.then ], [ 2, %if.else ]<br>  %tmp16 = phi i32 [ 3, %if.then ], [ 4, %if.else ]<br>  %add = add nsw i32 %tmp13, %tmp16<br>  ret i32 %add<br>}<br></pre><br>Here you can see that GVN found the available values, inserted PHI nodes, and eliminated the loads.<br><br><span style=font-size:x-large>More Complex Address Expressions</span><br><br>While the example above hopefully makes intuitive sense, it turns out that PHI translating expressions like this is actually a bit more difficult than it looks. It was so much so that it got split out into its own class, <a href=http://llvm.org/doxygen/classllvm_1_1PHITransAddr.html>PHITransAddr</a> (in <a href=http://llvm.org/doxygen/PHITransAddr_8h-source.html>llvm/Analysis/PHITransAddr.h</a>). Doing this was actually motivated by a few cute little examples like this:<br><br><pre>void test(int N, double* G) {<br>  for (long j = 1; j &lt; 1000; j++)<br>    G[j] = G[j] + G[j-1];<br>}<br></pre><br>This example&nbsp;(which was reduced from a larger example)&nbsp;has a loop carried redundancy where every iteration reads the previous iteration's value. In fact, the code could be rewritten like this, which only has one load in the loop:<br><br><pre>void test(int N, double* G) {<br>  double Prev = G[0];<br>  for (long j = 1; j &lt; 1000; j++) {<br>    Prev = G[j] + Prev;<br>    G[j] = Prev;<br>  }<br>}<br></pre><br>Some compilers have specific optimization passes that identify and eliminate these recurrences through dependence analysis. While this is a nice thing to do, sufficiently smart partial redundancy elimination of loads should also be able to eliminate this, and LLVM now does.<br><br>The unoptimized code looks like this in LLVM IR:<br><br><pre>define void @test(i32 %N, double* %G) {<br>bb.nph:<br>  br label %for.body<br><br>for.body:<br>  %indvar = phi i64 [ 0, %bb.nph ], [ %tmp, %for.body ] ; <b>indvar = [0 ... 999]</b><br>  %arrayidx6 = getelementptr double* %G, i64 %indvar<br>  %tmp = add i64 %indvar, 1<br>  %arrayidx = getelementptr double* %G, i64 %tmp<br>  %tmp3 = load double* %arrayidx                  ; <b>load G[indvar+1]</b><br>  %tmp7 = load double* %arrayidx6                 ; <b>load G[indvar]</b><br>  %add = fadd double %tmp3, %tmp7<br>  store double %add, double* %arrayidx            ; <b>store G[indvar+1]</b><br>  %exitcond = icmp eq i64 %tmp, 999<br>  br i1 %exitcond, label %for.end, label %for.body<br><br>for.end:<br>  ret void<br>}<br></pre><br>One interesting thing to be aware of here is that the -indvars<span style="font-family:courier new,Courier,monospace">&nbsp;</span>pass rewrote the induction variable to count from 0 to 999 instead of from 1 to 1000. &nbsp;This is just a canonicalization, but it is why we see a store to indvar+1 instead of directly through indvar.<br><br>In order to eliminate the redundant "<b>load G[indvar]</b>", GVN starts by looking for the dependencies of the %tmp7 load. &nbsp;It scans up the block, over several instructions that obviously don't modify the memory value, until it gets to the %arrayidx6 instruction, which defines the value. &nbsp;At this point it has to either stop phi translation (correct, but not very aggressive) or incorporate it into the scan and start looking for "gep %G, %indvar". &nbsp;It does this, until the next instruction, which defines %indvar. &nbsp;Since it found a definition of one of the inputs, it either has to phi translate (again, correct, not not very aggressive) or try to incorporate the value.<br><br>In this case, PHITransAddr attempts to translate the&nbsp;"gep %G, %indvar" symbolic expression across the PHI for each predecessor. &nbsp;In the %for.body predecessor, it looks to see if there is an instruction that produces the value "gep %G, %tmp" (because %tmp is the value of the PHI in the %for.body predecessor), and finds that this expression exists as %arrayidx. &nbsp;Since PHI translation succeeded, it scans from the bottom of the block to see if there is a definition of the value at the address %arrayidx, and finds that the %tmp3 load produces the desired value.<br><br>In the other predecessor (%bb.nph),&nbsp;PHITransAddr translates the symbolic "gep %G, %indvar" expression into "gep %G, 0" (which it uses InstSimplify to simplify down to "%G") and then checks to see if it is available as an address. &nbsp;"%G" is in fact a value address, so it scans from the bottom of the %bb.nph block to see if the value at address %G is available. &nbsp;In this case it isn't, so it records that the value is not available in the entry block, and that the phi translated address in that block is %G.<br><br>At this point, the recursive walk of the CFG has completed, and GVN knows that the %tmp7 load is in fact available as the %tmp3 value in one predecessor, but it is not available in the other predecessor. &nbsp;This is a classic case of a partial redundancy. &nbsp;Since the value is redundant on one edge but not the other, GVN makes the value fully redundant by inserting the computation (a load of %G) into the %bb.nph block, constructs SSA and eliminates the original load.<br><br>The final LLVM IR we get is:<br><br><pre>define void @test(i32 %N, double* %G) {<br>bb.nph:<br>  %tmp7.pre = load double* %G                     ; <b>Inserted by PRE</b><br>  br label %for.body<br><br>for.body:<br>  %tmp7 = phi double [ %tmp7.pre, %bb.nph ],<br>                     [ %add, %for.body ]          ; <b>SSA Construction</b><br>  %indvar = phi i64 [ 0, %bb.nph ], [ %tmp, %for.body ]<br>  %arrayidx6 = getelementptr double* %G, i64 %indvar<br>  %tmp = add i64 %indvar, 1<br>  %arrayidx = getelementptr double* %G, i64 %tmp<br>  %tmp3 = load double* %arrayidx<br>  %add = fadd double %tmp3, %tmp7      ; <b>Now uses the PHI instead of a load</b><br>  store double %add, double* %arrayidx<br>  %exitcond = icmp eq i64 %tmp, 999<br>  br i1 %exitcond, label %for.end, label %for.body<br><br>for.end:<br>  ret void<br>}<br></pre><br>Through this, GVN and PHI translation have worked together to eliminate a load in the loop. &nbsp;If you're more comfortable reading X86 machine code, here is the before and after code for the loop:<br><br>Before:<br><pre>LBB1_1:<br> movsd 8(%rsi,%rax,8), %xmm0   # Load<br> addsd (%rsi,%rax,8), %xmm0    # Load<br> movsd %xmm0, 8(%rsi,%rax,8)   # Store<br> incq %rax<br> cmpq $999, %rax<br> jne LBB1_1<br></pre><br>After:<br><span style=font-family:monospace;white-space:pre>LBB1_1:</span><br><pre>addsd 8(%rsi,%rcx,8), %xmm0   # Load<br> movsd %xmm0, 8(%rsi,%rcx,8)   # Store<br> incq %rax<br> incq %rcx<br> cmpq $999, %rcx<br> jne LBB1_1<br></pre><br>If you're interested in other examples of PHI translation, take a look at&nbsp;test/Transforms/GVN/rle-phi-translate.ll and&nbsp;test/Transforms/GVN/rle.ll in the LLVM distribution.<br><br><span style=font-size:x-large>Division of Labor</span><br><br>Getting this to work requires a number of different LLVM subsystems to work together. &nbsp;Here are the major players:<br><br><b>PHI Translation</b><br><br>The&nbsp;<a href=http://llvm.org/doxygen/classllvm_1_1PHITransAddr.html>PHITransAddr</a>&nbsp;class is the one that is responsible for building and translating symbolic expression like "gep %P, 1, (add %i, 1))" through PHI nodes. &nbsp;When the "instruction scan" finds a definition of an input to the current PHITransAddr expression, it either has to incorporate it into a (potentially larger and more complex) expression or give up. &nbsp;If it gives up, then PHI translation fails, otherwise it can keep scanning.<br><br><b>Memory Dependence Analysis</b><br><br>"MemDep" is the pass that does the CFG and instruction scanning. &nbsp;It builds a lazy and cached representation that is morally similar to "Virtual SSA Form" used by some other compilers, but is significantly more efficient than the virtual SSA forms that I'm aware of.<br><br>It exposes two major interfaces:<br><br>1) "give me all the local dependences of this load". &nbsp;This query scans the block the load lives in for dependent instructions. If it does not find a dependent instruction it returns "nonlocal" to indicate that the loaded memory value is potentially live-in to the block.<br><br>2) "give me all the non-local dependences for a load that is live in to a block". This query does the recursive upwards CFG scan that does PHI translation to find other definitions of the value.<br><br>In the interesting cases for PHI translation, we end up doing a non-local query and get back a set of blocks where the value is available along with a set of blocks where the value isn't available.<br><br><b>Alias Analysis:</b><br><br><a href=http://llvm.org/docs/AliasAnalysis.html>Alias analysis</a> (in this case, the -basicaa pass) is the underlying analysis that tells us whether two pointers can point to the same memory location or whether an instruction can modify or read from a memory location. &nbsp;This is required to allow MemDep to scan beyond stores and calls that do not clobber the address we're interested in.<br><br><b>SSA Update:</b><br><br>The <a href=http://llvm.org/doxygen/SSAUpdater_8h-source.html>SSAUpdater</a> class is used to insert PHI nodes based on the set of loads that we find are available.<br><br><b>GVN</b>:<br><br>The GVN pass is the transformation sitting on top of all of these subsystems. &nbsp;Because it is based on these other facilities, its logic is relatively simple: First, do a non-local memdep query. &nbsp;If it returns a set of definitions with no clobbers, then the load is fully redundant and can be eliminated. &nbsp;Otherwise, if there are some definitions live in, we have a partially redundant case. &nbsp;GVN handles inserting the new computation to make the value fully redundant.<br><br><div style=margin-bottom:0;margin-left:0;margin-right:0;margin-top:0><span style=font-size:x-large>Limitations</span><br></div><br>While there is a lot of power here, there are still some significant limitations to this system. &nbsp;As of this writing, here are some of the most significant ones:<br><br>First, an symbolic expression address must exist as an SSA value for PHI translation to succeed. &nbsp;In the last example, if we tried to phi translate "gep %G, %indvar" into a predecessor value which formed the symbolic expression of "gep %G, %xxx" and that symbolic expression did not actually exist in the code, PHI translation would fail. &nbsp;This happens because because after PHI translation occurs, we need MemDep to scan the block to find dependent instructions. &nbsp;Since MemDep queries are based on pointer values expressed as LLVM 'Value*'s, we have to have one to do the query.<br><br>Second, critical edges currently block PRE of loads because we do not want to introduce the load on a control flow path where it would not exist before. &nbsp;In principle, we could do this by lazily splitting the edge, but this would require updating the other in-flight data structures that the GVN pass is maintaining and we don't do this yet.<br><br>Finally, our PRE of loads can certainly be improved. &nbsp;Currently we only do PRE in cases where it would not grow the code and not introduce a computation on a path where it wouldn't exist before. &nbsp;Since we are deleting a load, this means that we only want to insert at most one load. &nbsp;The heuristic we use to determine whether this is the case is currently very local and can be improved.<br><br>In any case, I hope this gives an useful overview of how this subsystem in LLVM works, and how it got better in what will be LLVM 2.7.<br><br>-Chris</article><section class=post-nav><ul><li><a href=https://blog.llvm.org/2009/12/dreaded-two-phase-name-lookup.html><i class="fa fa-chevron-circle-left"></i> The Dreaded Two-Phase Name Lookup</a></li><li><a href=https://blog.llvm.org/2009/12/clang-builds-llvm.html>Clang Builds LLVM <i class="fa fa-chevron-circle-right"></i></a></li></ul></section></main><footer><ul><li><h6>llvm.org |
Rendered by <a href=https://gohugo.io title=Hugo>Hugo</a> |
<a href=https://blog.llvm.org/index.xml>Subscribe</a></h6></li></ul></footer></div><script src=https://blog.llvm.org/js/scripts.js></script></body></html>